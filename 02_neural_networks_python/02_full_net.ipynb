{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ProgramComputer/ai-science-training-series/blob/main/02_neural_networks_python/02_full_net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "662a93d1",
      "metadata": {
        "id": "662a93d1"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e19878bb",
      "metadata": {
        "id": "e19878bb"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "da412dba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da412dba",
        "outputId": "81ba08fd-602b-40b2-ec7d-397e6c9e8030"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n",
            "(60000, 28, 28)\n",
            "(60000, 784)\n",
            "\n",
            "MNIST data loaded: train: 60000 test: 10000\n",
            "X_train: (60000, 784)\n",
            "y_train: (60000,)\n"
          ]
        }
      ],
      "source": [
        "# repeating the data prep from the previous notebook\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.astype(numpy.float32)\n",
        "x_test  = x_test.astype(numpy.float32)\n",
        "\n",
        "x_train /= 255.\n",
        "x_test  /= 255.\n",
        "\n",
        "print(x_train.shape)\n",
        "x_train = x_train.reshape(x_train.shape[0], numpy.prod(x_train[0,:,:].shape))\n",
        "x_test = x_test.reshape(x_test.shape[0], numpy.prod(x_test[0,:,:].shape))\n",
        "\n",
        "print(x_train.shape)\n",
        "y_train = y_train.astype(numpy.int32)\n",
        "y_test  = y_test.astype(numpy.int32)\n",
        "\n",
        "print()\n",
        "print('MNIST data loaded: train:',len(x_train),'test:',len(x_test))\n",
        "print('X_train:', x_train.shape)\n",
        "print('y_train:', y_train.shape)\n",
        "\n",
        "# one-hot encoding:\n",
        "nb_classes = 10\n",
        "y_train_onehot = tf.keras.utils.to_categorical(y_train, nb_classes)\n",
        "y_test_onehot = tf.keras.utils.to_categorical(y_test, nb_classes)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/ProgramComputer/ai-science-training-series/main/02_neural_networks_python/fc_net.py\n",
        "!wget https://raw.githubusercontent.com/ProgramComputer/ai-science-training-series/main/02_neural_networks_python/layer_utils.py\n",
        "!wget https://raw.githubusercontent.com/ProgramComputer/ai-science-training-series/main/02_neural_networks_python/layers.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bCRqW__pASv",
        "outputId": "b7764bd9-f556-4d13-e525-8b03f14b518c"
      },
      "id": "7bCRqW__pASv",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-14 19:36:48--  https://raw.githubusercontent.com/ProgramComputer/ai-science-training-series/main/02_neural_networks_python/fc_net.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3920 (3.8K) [text/plain]\n",
            "Saving to: ‘fc_net.py.1’\n",
            "\n",
            "\rfc_net.py.1           0%[                    ]       0  --.-KB/s               \rfc_net.py.1         100%[===================>]   3.83K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-11-14 19:36:48 (45.2 MB/s) - ‘fc_net.py.1’ saved [3920/3920]\n",
            "\n",
            "--2022-11-14 19:36:48--  https://raw.githubusercontent.com/ProgramComputer/ai-science-training-series/main/02_neural_networks_python/layer_utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 898 [text/plain]\n",
            "Saving to: ‘layer_utils.py’\n",
            "\n",
            "layer_utils.py      100%[===================>]     898  --.-KB/s    in 0s      \n",
            "\n",
            "2022-11-14 19:36:48 (47.8 MB/s) - ‘layer_utils.py’ saved [898/898]\n",
            "\n",
            "--2022-11-14 19:36:48--  https://raw.githubusercontent.com/ProgramComputer/ai-science-training-series/main/02_neural_networks_python/layers.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3698 (3.6K) [text/plain]\n",
            "Saving to: ‘layers.py’\n",
            "\n",
            "layers.py           100%[===================>]   3.61K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-11-14 19:36:48 (48.7 MB/s) - ‘layers.py’ saved [3698/3698]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "302994b1",
      "metadata": {
        "id": "302994b1"
      },
      "outputs": [],
      "source": [
        "# Here we import an implementation of a two-layer neural network \n",
        "# this code is based on pieces of the first assignment from Stanford's CSE231n course, \n",
        "# hosted at https://github.com/cs231n/cs231n.github.io with the MIT license\n",
        "from fc_net import TwoLayerNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4e00e3de",
      "metadata": {
        "id": "4e00e3de"
      },
      "outputs": [],
      "source": [
        "num_features = x_train.shape[1] # this is the number of pixels\n",
        "# The weights are initialized from a normal distribution with standard deviation weight_scale\n",
        "model = TwoLayerNet(input_dim=num_features, hidden_dim=300, num_classes=nb_classes, weight_scale=.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "32f7f1aa",
      "metadata": {
        "id": "32f7f1aa"
      },
      "outputs": [],
      "source": [
        "# here you can take a look if you want at the initial loss from an untrained network\n",
        "loss, gradients = model.loss(x_train, y_train_onehot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "c43e3aa5",
      "metadata": {
        "id": "c43e3aa5"
      },
      "outputs": [],
      "source": [
        "# a simple implementation of stochastic gradient descent\n",
        "def sgd(model, gradients, learning_rate):\n",
        "    for p, w in model.params.items():\n",
        "        dw = gradients[p]\n",
        "        new_weights = w - learning_rate * dw\n",
        "        model.params[p] = new_weights\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c8316228",
      "metadata": {
        "id": "c8316228"
      },
      "outputs": [],
      "source": [
        "# one training step\n",
        "def learn(model, x_train, y_train_onehot, learning_rate):\n",
        "    loss, gradients = model.loss(x_train, y_train_onehot)\n",
        "    model = sgd(model, gradients, learning_rate)\n",
        "    return loss, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "81886e8c",
      "metadata": {
        "id": "81886e8c"
      },
      "outputs": [],
      "source": [
        "def accuracy(model, x, true_values):\n",
        "    scores = model.loss(x)\n",
        "    predictions = numpy.argmax(scores, axis=1)\n",
        "    N = predictions.shape[0]\n",
        "    acc = (true_values == predictions).sum() / N\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def training(learning_rate, batch_size, num_epochs, hidden_dim, weight_scale, x_train, y_train, y_train_onehot, x_val, y_val, y_val_onehot):\n",
        "    num_features = x_train.shape[1] # this is the number of pixels\n",
        "    nb_classes = y_train_onehot.shape[1]\n",
        "    \n",
        "    # The weights are initialized from a normal distribution with standard deviation weight_scale\n",
        "    model = TwoLayerNet(input_dim=num_features, hidden_dim=hidden_dim, num_classes=nb_classes, weight_scale=weight_scale)\n",
        "    \n",
        "    num_examples_train = x_train.shape[0]\n",
        "    num_batches_train = int(num_examples_train / batch_size)\n",
        "    \n",
        "    losses = numpy.zeros((num_batches_train*num_epochs,2)) # 1st column for training data, 2nd for validation data\n",
        "    # EDIT: also track accuracy\n",
        "    accuracies = numpy.zeros((num_batches_train*num_epochs,2)) \n",
        "    \n",
        "    indices_train = numpy.arange(num_examples_train)\n",
        "    num_examples_val = x_val.shape[0]\n",
        "    indices_val = numpy.arange(num_examples_val)\n",
        "\n",
        "    i = 0\n",
        "    for epoch in range(0, num_epochs):\n",
        "        start_epoch = time.time()\n",
        "        # in each epoch, we loop over all of the training examples\n",
        "        for step in range(0, num_batches_train):\n",
        "            # grabbing the next training batch\n",
        "            offset_train = step * batch_size\n",
        "            batch_range_train = range(offset_train, offset_train+batch_size)\n",
        "            x_train_batch = x_train[batch_range_train, :]\n",
        "            # EDIT: keep around non-onehot labels as well for accuracy calculation\n",
        "            y_train_onehot_batch = y_train_onehot[batch_range_train,:]\n",
        "            y_train_batch = y_train[batch_range_train,numpy.newaxis]\n",
        "\n",
        "            # one approach: grab a random validation batch (random offset into number of validation examples)\n",
        "            offset_val = numpy.random.randint(low=0, high=num_examples_val-batch_size)\n",
        "            batch_range_val = range(offset_val, offset_val+batch_size)\n",
        "            x_val_batch = x_val[batch_range_val, :]\n",
        "            y_val_onehot_batch = y_val_onehot[batch_range_val,:]\n",
        "            y_val_batch = y_val[batch_range_val,numpy.newaxis]\n",
        "\n",
        "            # feed the next batch in to do one sgd step\n",
        "            loss_train = learn(model, x_train_batch, y_train_onehot_batch, learning_rate)\n",
        "\n",
        "            # check training & validation loss & accuracy\n",
        "            losses[i,0] = loss_train\n",
        "            \n",
        "            # could save time by commenting out the next three lines and only tracking at the epoch level\n",
        "            accuracies[i,0] = accuracy(model, x_train_batch, y_train_batch)\n",
        "            losses[i,1], _ = model.loss(x_val_batch, y_val_onehot_batch)\n",
        "            accuracies[i,1] = accuracy(model, x_val_batch, y_val_batch)\n",
        "            i += 1\n",
        "\n",
        "        # slower, so we're only doing this once per epoch: checking accuracy on all of the data at once\n",
        "        acc_train = accuracy(model, x_train, y_train)\n",
        "        acc_val = accuracy(model, x_val, y_val)\n",
        "        \n",
        "        # reshuffle the data so that we get a new set of batches\n",
        "        numpy.random.shuffle(indices_train)\n",
        "        x_train = x_train[indices_train,:]\n",
        "        y_train = y_train[indices_train] # keep this shuffled the same way for use in accuracy calculation\n",
        "        y_train_onehot = y_train_onehot[indices_train,:]\n",
        "\n",
        "        numpy.random.shuffle(indices_val)\n",
        "        x_val = x_val[indices_val,:]\n",
        "        y_val = y_val[indices_val] \n",
        "        y_val_onehot = y_val_onehot[indices_val,:]\n",
        "        end_epoch = time.time()\n",
        "        time_this_epoch = end_epoch - start_epoch\n",
        "        print(\"epoch %d took %.1f seconds, training loss %.5f (last batch), training accuracy %.3f, validation accuracy %.3f\" % (epoch, time_this_epoch, loss_train, acc_train, acc_val))\n",
        "    return losses, accuracies, model\n"
      ],
      "metadata": {
        "id": "2fZv76xJqadG"
      },
      "id": "2fZv76xJqadG",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49754891",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49754891",
        "outputId": "2a6fc1c8-90e3-4ca1-f0c0-abf69d5e3480"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, loss 2.28467, accuracy 0.46\n",
            "epoch 1, loss 2.28262, accuracy 0.47\n"
          ]
        }
      ],
      "source": [
        "# Here's an example training loop using this two-layer model. Can you do better? \n",
        "learning_rate = 0.01  \n",
        "num_examples = x_train.shape[0]\n",
        "batch_size = 10000\n",
        "num_batches = int(num_examples / batch_size)\n",
        "num_epochs = 10\n",
        "losses = numpy.zeros(num_batches*num_epochs,)\n",
        "indices = numpy.arange(num_examples)\n",
        "\n",
        "i = 0\n",
        "for epoch in range(0, num_epochs):\n",
        "    # in each epoch, we loop over all of the training examples\n",
        "    for step in range(0, num_batches):\n",
        "        # grabbing the next batch\n",
        "        offset = step * batch_size\n",
        "        batch_range = range(offset, offset+batch_size)\n",
        "        x_train_batch = x_train[batch_range, :]\n",
        "        y_train_batch = y_train_onehot[batch_range,:]\n",
        "        \n",
        "        # feed the next batch in to do one sgd step\n",
        "        loss, model = learn(model, x_train_batch, y_train_batch, learning_rate)\n",
        "        losses[i] = loss\n",
        "        i += 1\n",
        "\n",
        "    acc = accuracy(model, x_train, y_train)\n",
        "    print(\"epoch %d, loss %.5f, accuracy %.2f\" % (epoch, loss, acc))\n",
        "    \n",
        "    # reshuffle the data so that we get a new set of batches\n",
        "    numpy.random.shuffle(indices)\n",
        "    x_train = x_train[indices,:]\n",
        "    y_train = y_train[indices] # keep this shuffled the same way for use in accuracy calculation\n",
        "    y_train_onehot = y_train_onehot[indices,:]\n",
        "plt.plot(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "a4f274c6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4f274c6",
        "outputId": "98d5a945-b5f3-4442-e5a2-8e3d949a6184"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4452333333333333"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "accuracy(model, x_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2dd5728",
      "metadata": {
        "id": "a2dd5728"
      },
      "source": [
        "# Homework: improve the accuracy of this model. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faaf0515",
      "metadata": {
        "id": "faaf0515"
      },
      "source": [
        "Update this notebook so that the accuracy is improved. How high can you get it? You could change things directly in the notebook, such as increasing the number of epochs, changing the learning weight, changing the width of the hidden layer, etc. If you're more ambitious, you could also try changing the model definition itself by checking out the associated Python files. For example, you could add more layers to the network. The current notebook has a training accuracy of about 43%, but will vary with randomness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "6e484c13",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "6e484c13",
        "outputId": "6343fb54-6286-4d48-e0fb-977cffb289cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.03210479558034334\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-8e4384dabe67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mweight_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mweight_scale1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mweight_scale2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_onehot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_onehot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'training' is not defined"
          ]
        }
      ],
      "source": [
        "learning_rate = 0.3\n",
        "batch_size = 1200\n",
        "num_epochs = 25\n",
        "hidden_dim = 1600\n",
        "# Try Xavier initialization, although need to use same scaling for both layers, so averaging\n",
        "# https://keras.io/api/layers/initializers/\n",
        "size_input = x_train.shape[1]\n",
        "weight_scale1 = numpy.sqrt(2./(size_input+hidden_dim))\n",
        "weight_scale2 = numpy.sqrt(2./(hidden_dim+nb_classes))\n",
        "weight_scale = (weight_scale1 + weight_scale2)/2\n",
        "print(weight_scale)\n",
        "losses, accuracies, model = training(learning_rate, batch_size, num_epochs, hidden_dim, weight_scale, x_train, y_train, y_train_onehot, x_val, y_val, y_val_onehot)\n",
        "\n",
        "plt.figure()\n",
        "plt.subplot(2,1,1)\n",
        "plt.plot(losses[:,0])\n",
        "plt.plot(losses[:,1])\n",
        "plt.legend([\"train\", \"val\"])\n",
        "\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(accuracies[:,0])\n",
        "plt.plot(accuracies[:,1])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}